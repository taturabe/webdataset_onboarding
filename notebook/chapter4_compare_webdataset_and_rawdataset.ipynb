{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f3a588",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "## WebDatasetのベンチマーク\n",
    "\n",
    "このチャプターでは、大量のnumpy配列ファイルのデータセットをWebDataset形式で作成し、個々のファイルを読み込む通常の形式のデータセットとパフォーマンスの比較をします。\n",
    "\n",
    "特に大規模データセットをクラウド上のストレージに置く場合に違いが顕著になります。\n",
    "\n",
    "このチャプターは比較的大きなデータセット（〜10GB）を扱うため、データの作成やS3への事前のアップロードに時間・コストがかかる場合があります。SageMaker notebookからの実行をお勧めします。また、データセットの容量や個数は環境に応じて適宜調整してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import webdataset as wds\n",
    "import tqdm\n",
    "import boto3\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32083da3",
   "metadata": {},
   "source": [
    "大規模データセット用のnumpy.arrayファイルとシャードされたtarファイルを格納するフォルダ名を指定し、初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca88240",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dir = \"npy_large\"\n",
    "shard_dir = \"shard_large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(arr_dir):\n",
    "    print(\"Deleting exitsting array directory. \")\n",
    "    shutil.rmtree(arr_dir)\n",
    "print(\"(re)creating new array directory\")\n",
    "os.makedirs(arr_dir)\n",
    "\n",
    "if os.path.exists(shard_dir):\n",
    "    print(\"Deleting exitsting shard directory. \")\n",
    "    shutil.rmtree(shard_dir)\n",
    "print(\"(re)creating new shard directory\")\n",
    "os.makedirs(shard_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084443f",
   "metadata": {},
   "source": [
    "numpy配列形式のデータセットを擬似的に作成します。\n",
    "\n",
    "入力データ`（X）`は倍精度の二次元配列、教師データ`(y)`は、倍精度のnumpy arrayです。\n",
    "各要素の値はサンプル名と同様にしています。\n",
    "\n",
    "作成した`X`および`y`をそれぞれ`.input.npy`, `.output.npy`の形式で保存します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = np.ones((64,64)) * 0\n",
    "    y = np.array(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d160cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000 # number of sample files\n",
    "X_shape = (64,64)\n",
    "\n",
    "recipe_fn = \"npy_recipe\"\n",
    "npy_recipe = open(recipe_fn, \"w\")\n",
    "\n",
    "for i in tqdm.tqdm(range(num_samples)):\n",
    "    # create input array\n",
    "    X = np.ones(X_shape) * i\n",
    "    # create outpu array\n",
    "    y = np.array(i)\n",
    "    \n",
    "    # set basename for npy input/output files\n",
    "    arr_fn = \"arr_%06d\" % i\n",
    "    # set npy filenames\n",
    "    X_name = f\"{arr_fn}.input.npy\"\n",
    "    y_name = f\"{arr_fn}.output.npy\"\n",
    "    # save array\n",
    "    np.save(os.path.join(arr_dir, X_name), X)\n",
    "    np.save(os.path.join(arr_dir, y_name), y)\n",
    "    \n",
    "    # write sample information onto recipe file\n",
    "    npy_recipe.write(f\"{X_name}\\tfile:{os.path.join(arr_dir, X_name)}\\n\")\n",
    "    npy_recipe.write(f\"{y_name}\\tfile:{os.path.join(arr_dir, y_name)}\\n\")\n",
    "\n",
    "npy_recipe.close()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9470d7b",
   "metadata": {},
   "source": [
    "保存した2種類のnumpy arrayファイル (`.input.npy`,`.output.npy`)からtarファイルを作成します。\n",
    "\n",
    "一つのtarファイルにまとめるのではなく、サンプル数ごとにシャードされたtarファイルに分割します。\n",
    "WebDatasetのTarWriterクラスを使うと、tarファイルを簡単に作ることができます。\n",
    "\n",
    "ここではarr_000000.input.npyとarr_000000.output.npyを例にとります。\n",
    "これらサンプルをWebDatasetに読み込めるためのtarファイルに保存するためには以下のような辞書形式のオブジェクト`sample`を作成します。\n",
    "\n",
    "\n",
    "`\"__key__\"`: 拡張子なしのベースネーム <BR>\n",
    "`\"{入力ファイルの拡張子}\"`:入力ファイルのバイトストリームデータ <BR>\n",
    "`\"{出力ファイルの拡張子}\"`:出力ファイルのバイトストリームデータ <BR>\n",
    "\n",
    "今回の場合、入力・出力ともnumpyファイル`.npy`であるため、両者を区別するために、拡張子の前に識別子を付け加えています。画像ファイルとjsonファイルのような場合は、\n",
    "\n",
    "\n",
    "`\"__key__\"`:sample_0000<BR>\n",
    "`\".jpeg\"`: jpegファイルのバイトストリームデータ<BR>\n",
    "`\".json\"`:jsonファイルのバイトストリームデータ<BR>\n",
    "    \n",
    "といった形式にしてください。\n",
    "\n",
    "ここでは、`sample_per_shard`変数によって一つのシャード(tarファイル)中のサンプルの数を指定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b515d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullnames = glob.glob(os.path.join(arr_dir, \"*.input.npy\"))\n",
    "fullnames.sort()\n",
    "\n",
    "sample_per_shard = 1000\n",
    "i = 0\n",
    "while i*sample_per_shard < len(fullnames):\n",
    "    print(f\"creating {i}-th tar file...\")\n",
    "    sink = wds.TarWriter(f\"{shard_dir}/npy_webdataset-%04d.tar\" % i, encoder=False)\n",
    "    fullnames_per_shard = fullnames[(i*sample_per_shard):((i+1)*sample_per_shard)]\n",
    "    for fullname in fullnames_per_shard:\n",
    "        fullname_wo_ext = fullname.split(\".\")[0]\n",
    "        basename_wo_ext = os.path.basename(fullname_wo_ext)\n",
    "    \n",
    "        #print(basename_wo_ext)\n",
    "        with open(f\"{fullname_wo_ext}.input.npy\", \"rb\") as stream:\n",
    "            inp = stream.read()\n",
    "        with open(f\"{fullname_wo_ext}.output.npy\", \"rb\") as stream:\n",
    "            out = stream.read()        \n",
    "        sample = {\n",
    "            \"__key__\": basename_wo_ext,\n",
    "            \"input.npy\": inp,\n",
    "            \"output.npy\": out\n",
    "        }\n",
    "        sink.write(sample)\n",
    "    sink.close()\n",
    "    i +=1\n",
    "print(\"Finished!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308b7e7",
   "metadata": {},
   "source": [
    "### S3へのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "パフォーマンスを比較するため、tarファイルに固めたデータと、元のnpyデータの両方をS3にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"put your bucket name\"\n",
    "prefix = \"put your prefix\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5752d",
   "metadata": {},
   "source": [
    "まず、作成したtarファイルをS3上にアップロードします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68878578",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --delete {shard_dir} s3://{bucket}/{prefix}/{shard_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058de0e",
   "metadata": {},
   "source": [
    "次に、元のnpyデータをS3上にアップロードします。<BR>\n",
    "**この作業は場合によっては1時間以上かかる場合があります。**\n",
    "時間がかかりすぎる場合はサンプル数を調節してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb654c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --delete {arr_dir} s3://{bucket}/{prefix}/{arr_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ae6d0",
   "metadata": {},
   "source": [
    "### WebDatasetのパフォーマンス比較"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e822286",
   "metadata": {},
   "source": [
    "作成したtarファイルからWebDatasetを作成します。作成したtarファイルの数に応じて、urlの`{0000..0099}`の部分を調整してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29767649",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"s3://{bucket}/{prefix}/{shard_dir}/\" + \"npy_webdataset-{0000..0099}.tar\"\n",
    "url = f\"pipe:aws s3 cp {url} - || true\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadac1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28589a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9341b0e",
   "metadata": {},
   "source": [
    "### バッチデータのフィード\n",
    "\n",
    "#### case1: num_batch=1; シャッフルなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75300ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time():\n",
    "    return datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"start: {print_time()}\")\n",
    "webdataset = wds.WebDataset(url, shardshuffle=False, cache_dir=None).decode()\n",
    "webdataset = webdataset.to_tuple(\"input.npy\", \"output.npy\")\n",
    "webloader = wds.WebLoader(webdataset, num_workers=0)\n",
    "print(f\"end: {print_time()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a89376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(f\"start: {print_time()}\")\n",
    "\n",
    "for i, (inp, out) in enumerate(webloader):\n",
    "    #print(out)\n",
    "    print('.', end='')\n",
    "    pass\n",
    "end = time.time()\n",
    "print(f\"total time: {(end-start)} sec\")\n",
    "print(f\"end: {print_time()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb8bc1",
   "metadata": {},
   "source": [
    "#### case 2: num_batch = 100; シャッフルなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"start: {print_time()}\")\n",
    "webdataset = wds.WebDataset(url, shardshuffle=False, cache_dir=None).decode()\n",
    "webdataset = webdataset.to_tuple(\"input.npy\", \"output.npy\").batched(100)\n",
    "webloader = wds.WebLoader(webdataset, num_workers=0)\n",
    "print(f\"end: {print_time()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e745d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(f\"start: {print_time()}\")\n",
    "\n",
    "for i, (inp, out) in enumerate(webloader):\n",
    "    #print(out)\n",
    "    print('.', end='')\n",
    "    pass\n",
    "end = time.time()\n",
    "print(f\"total time: {(end-start)} sec\")\n",
    "print(f\"end: {print_time()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7666c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://taturabe-dataset/dataset/webdataset/shard_large/npy_webdataset-0099.tar tmp.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b015c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client('athena', region_name='us-east-1')\n",
    "\n",
    "query =f'''\n",
    "select\n",
    "\n",
    "    eventtime,\n",
    "    eventname,\n",
    "    json_extract_scalar(requestparameters, '$.bucketName') as bucketName,\n",
    "    json_extract_scalar(requestparameters, '$.key') as key\n",
    "\n",
    "from \n",
    "    cloudtrail_logs_aws_cloudtrail_logs_820974724107_798abb34\n",
    "WHERE eventName='GetObject'\n",
    "    AND eventTime BETWEEN '{start}' and '{end}'\n",
    "ORDER BY eventtime DESC\n",
    "'''\n",
    "\n",
    "query_exec = athena.start_query_execution(\n",
    "QueryString=query,\n",
    "QueryExecutionContext={\n",
    "'Database': 'default'\n",
    "},\n",
    "ResultConfiguration={\n",
    "'OutputLocation': 's3://taturabe-dataset/tmp/'\n",
    "}\n",
    ")\n",
    "    \n",
    "query_id = query_exec['QueryExecutionId']\n",
    "\n",
    "\n",
    "state=None\n",
    "while(True):\n",
    "    if state=='SUCCEEDED':\n",
    "        print(\"query succeeded\")\n",
    "        break\n",
    "    elif state=='FAILED':\n",
    "        print(\"query failed!!\")\n",
    "        break\n",
    "    time.sleep(1)\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "    query_exec = athena.get_query_execution(\n",
    "                                QueryExecutionId=query_id\n",
    "                                )\n",
    "    state = query_exec['QueryExecution']['Status']['State']\n",
    "\n",
    "queryres = athena.get_query_results(\n",
    "                        QueryExecutionId=query_id\n",
    "                        )\n",
    "csv_path=query_exec['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "query_df =pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb63e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    time.sleep(1)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ec55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63407829",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = athena.get_query_execution(\n",
    "QueryExecutionId=query_id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbbe97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "webdataset = wds.WebDataset(url, shardshuffle=False, cache_dir=None).decode()\n",
    "webdataset = webdataset.to_tuple(\"input.npy\", \"output.npy\").batched(32)\n",
    "webloader = wds.WebLoader(webdataset, num_workers=os.cpu_count()-1)\n",
    "start = time.time()\n",
    "for i, (inp, out) in enumerate(webloader):\n",
    "    #print(out)\n",
    "    print('.', end='')\n",
    "    pass\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"total time: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0367914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3のprefix中のフォルダを検索する\n",
    "def get_all_s3_objects(s3, **base_kwargs):\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        list_kwargs = dict(MaxKeys=1000, **base_kwargs)\n",
    "        if continuation_token:\n",
    "            list_kwargs['ContinuationToken'] = continuation_token\n",
    "        response = s3.list_objects_v2(**list_kwargs)\n",
    "        yield from response.get('Contents', [])\n",
    "        if not response.get('IsTruncated'):  # At the end of the list?\n",
    "            break\n",
    "        continuation_token = response.get('NextContinuationToken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cad4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "contents = list(get_all_s3_objects(boto3.client('s3'), Bucket=bucket, Prefix=os.path.join(prefix, arr_dir)))\n",
    "\n",
    "input_arr_path = [c['Key'] for c in contents if c['Key'][-9:] == \"input.npy\"]\n",
    "basename_path = [i.split(\".input.npy\")[0] for i in input_arr_path] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(basename_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03682451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_images_path_list, client, bucket):        \n",
    "        self.input_images_path_list = input_images_path_list\n",
    "        self.client = client\n",
    "        self.bucket = bucket\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.input_images_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # download .npy array from S3\n",
    "        obj = self.client.get_object(Bucket=self.bucket, Key=self.input_images_path_list[idx] + \".input.npy\")\n",
    "        input_arr = np.frombuffer( obj['Body'].read(), dtype='float64', offset=128)\n",
    "        input_arr = input_arr.reshape(64,64)\n",
    "    \n",
    "    \n",
    "        obj = self.client.get_object(Bucket=self.bucket, Key=(self.input_images_path_list[idx]) + \".output.npy\")\n",
    "        output_arr = np.frombuffer( obj['Body'].read(), dtype='float64', offset=128)\n",
    "        output_arr = output_arr.reshape(1)        \n",
    "\n",
    "        input_tensor = torch.from_numpy(input_arr)\n",
    "        output_tensor = torch.from_numpy(output_arr)\n",
    "\n",
    "        #### 要修正\n",
    "        # data typeの変更 float16は未対応\n",
    "        input_tensor = input_tensor.type(torch.float32)\n",
    "        output_tensor = output_tensor.type(torch.float32)\n",
    "\n",
    "\n",
    "        return input_tensor, output_tensor, self.input_images_path_list[idx]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56381856",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3dataset = S3Dataset(basename_path, s3_client, bucket)\n",
    "s3loader =  torch.utils.data.DataLoader(s3dataset, batch_size=32, \n",
    "                                        shuffle=False, num_workers=os.cpu_count()-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf49116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i, (inp,out,path) in enumerate(s3loader):\n",
    "    print(path)\n",
    "    #print('.', end='')\n",
    "end = time.time()\n",
    "print(\"total time: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04b80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
